from core import JobScraper, JobParser, save_to_csv
import re
from bs4 import BeautifulSoup

def detect_max_pages(scraper):
    
    print("A detectar n√∫mero m√°ximo de p√°ginas...")
    
    try:
        scraper.init_driver()
        
        # Vai para a primeira p√°gina
        scraper.driver.get(scraper.base_url + "?page=1")
        
        # Parse do HTML
        soup = BeautifulSoup(scraper.driver.page_source, "html.parser")
        
        max_pages = 1
        
        # Estrat√©gia 1: Procurar por pagina√ß√£o
        pagination_selectors = [
            ".pagination a",           # Links de pagina√ß√£o comuns
            ".page-link",             # Bootstrap pagination
            ".pagination-list a",     # Outro padr√£o comum
            "a[href*='page=']"        # Qualquer link com page=
        ]
        
        for selector in pagination_selectors:
            page_links = soup.select(selector)
            if page_links:
                print(f"Encontrados {len(page_links)} links de pagina√ß√£o com seletor: {selector}")
                
                # Extrai n√∫meros de p√°gina dos links
                page_numbers = []
                for link in page_links:
                    href = link.get('href', '')
                    text = link.get_text(strip=True)
                    
                    # Procura por page= no href
                    page_match = re.search(r'page=(\d+)', href)
                    if page_match:
                        page_numbers.append(int(page_match.group(1)))
                    
                    # Procura por n√∫mero no texto do link
                    if text.isdigit():
                        page_numbers.append(int(text))
                
                if page_numbers:
                    max_from_links = max(page_numbers)
                    max_pages = max(max_pages, max_from_links)
                    print(f"   üìä Maior n√∫mero de p√°gina encontrado nos links: {max_from_links}")
                break
        
        # Estrat√©gia 2: Procurar por informa√ß√£o de resultados
        result_info_selectors = [
            ".results-info",
            ".search-results-info", 
            ".pagination-info",
            ".total-results"
        ]
        
        for selector in result_info_selectors:
            info_element = soup.select_one(selector)
            if info_element:
                info_text = info_element.get_text()
                print(f"Informa√ß√£o de resultados: {info_text}")
                
                # Procura por padr√µes como "P√°gina 1 de 15" ou "1-20 de 300 resultados"
                page_pattern = re.search(r'(\d+)\s*de\s*(\d+)', info_text)
                if page_pattern:
                    total_pages = int(page_pattern.group(2))
                    max_pages = max(max_pages, total_pages)
                    print(f"P√°ginas totais encontradas na informa√ß√£o: {total_pages}")
                break
        
        # Estrat√©gia 3: Testar p√°ginas incrementalmente (sem limite)
        if max_pages <= 3: 
            print("A testar p√°ginas incrementalmente...")
            test_page = 2
            
            while True:  # Sem limite - testa at√© n√£o encontrar mais p√°ginas
                test_url = f"{scraper.base_url}?page={test_page}"
                scraper.driver.get(test_url)
                
                test_soup = BeautifulSoup(scraper.driver.page_source, "html.parser")
                
                # Verifica se h√° ofertas na p√°gina
                job_selectors = [
                    ".job-item",
                    ".offer-item", 
                    ".job-listing",
                    "article",
                    ".search-result"
                ]
                
                has_jobs = False
                for job_selector in job_selectors:
                    if test_soup.select(job_selector):
                        has_jobs = True
                        break
                
                if not has_jobs:
                    # Se n√£o h√° ofertas, a p√°gina anterior era a √∫ltima
                    max_pages = test_page - 1
                    print(f"P√°gina {test_page} est√° vazia. Total de p√°ginas: {max_pages}")
                    break
                
                test_page += 1
                
                # Mostra progresso a cada 10 p√°ginas
                if test_page % 10 == 0:
                    print(f"Testada p√°gina {test_page-1}... continuando")
                
                # Seguran√ßa extrema - parar se chegar a 1000 p√°ginas (improv√°vel)
                if test_page > 1000:
                    max_pages = 1000
                    print(f"Limite extremo de seguran√ßa atingido: {max_pages} p√°ginas")
                    break
        
        scraper.driver.quit()
        
        print(f"Detec√ß√£o conclu√≠da! Total de p√°ginas encontradas: {max_pages}")
        return max_pages
        
    except Exception as e:
        print(f"Erro na detec√ß√£o de p√°ginas: {e}")
        if scraper.driver:
            scraper.driver.quit()
        return 10

def main():
    print("Iniciando o JobScraper-Portugal...")

    # Inicializa o scraper
    scraper = JobScraper(base_url="https://www.itjobs.pt/ofertas")

    # Detecta automaticamente o n√∫mero m√°ximo de p√°ginas
    max_pages = detect_max_pages(scraper)
    
    print(f"\nA processar {max_pages} p√°ginas...")
    
    # Faz o download do HTML das p√°ginas de ofertas
    pages = scraper.get_job_pages(num_pages=max_pages)

    # Extrai dados completos visitando p√°ginas individuais para melhor precis√£o
    print("Modo completo ativado - visitando p√°ginas individuais para melhor precis√£o...")
    if max_pages > 50:
        print("AVISO: Com muitas p√°ginas, este processo pode demorar v√°rias horas!")
    
    raw_jobs = scraper.extract_raw_jobs(pages, visit_individual_pages=True)

    # Analisa e organiza os dados
    parser = JobParser()
    parsed_jobs = parser.parse_jobs(raw_jobs)

    # Guarda num CSV com nome que inclui o n√∫mero de p√°ginas
    filename = f"data/jobs_itjobs_max_{max_pages}pages.csv"
    save_to_csv(parsed_jobs, filename)

    print("Conclu√≠do! Dados guardados em '{}'".format(filename))
    print(f"Estat√≠sticas:")
    print(f"   ‚Ä¢ {max_pages} p√°ginas processadas")
    print(f"   ‚Ä¢ {len(parsed_jobs)} ofertas extra√≠das")
    print(f"   ‚Ä¢ M√©dia de {len(parsed_jobs)/max_pages:.1f} ofertas por p√°gina")
    
    # Estat√≠sticas detalhadas
    fields_with_data = {}
    for field in ["T√≠tulo", "Empresa", "Localiza√ß√£o", "Tipo de contrato", "Seniority", "Tecnologias", "Modo de trabalho", "Categoria", "Data de publica√ß√£o", "Descri√ß√£o"]:
        count = sum(1 for job in parsed_jobs if job.get(field, "N/A") != "N/A")
        fields_with_data[field] = count
        percentage = count/len(parsed_jobs)*100 if len(parsed_jobs) > 0 else 0
        print(f"   ‚Ä¢ {field}: {count}/{len(parsed_jobs)} ({percentage:.1f}%)")

if __name__ == "__main__":
    main()